#### If supervised:

cluster's diversity index (information entropy)

### Shannon information measure of entropy:
#### $$E(C_i)=-\sum^{|labels|}_{j=1}p_{i,j}log_2p_{i,j}$$ $$\text{where~}p_{i,j}= \frac{count_{i,j}}{count_{i}}$$ then mean entropy = $$\bar{E}(C)=\sum^{k}_{1}\frac{||C_i||}{||M||}E(c_i)$$
Maximize M


### 1D Gaussian mixture model
Derivative of Error function: 
### $$\sigma^2_{ML}=$$

### Expectation Maximization Algorithm

Still Expensive to calculate 
