#### Assumptions:
- Data uniformly distributed
- Hypothesis on free variable w/in range
- N measurements > parameters
- Free variable continuous
- ...

### [[Gradient Descent]] Optimization
Model: $h_b(x)=b_0+b_1x$
Parameters: $b_0,b_1$
Cost function: $J(b_0,b_1)=1/(2m)+\sum_{i=1}^{m}{h_b(x^i-y^i)^2}$
Gradient descent (goal): minimize$_{b0,b1}J(b0,b1)$

### [[Kernel Methods]]
